{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langmem 0.0.4 requires httpx<0.27.0,>=0.26.0, but you have httpx 0.28.1 which is incompatible.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/Users/rahulbhoyar/Projects/DataScience/GenAI-with-Python/venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q langgraph langchain langchain_ollama pydantic typing-inspect langchain-core ipython langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ollama serve > /dev/null 2>&1 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kh9xMYqYrtoS",
    "outputId": "9ffeba3e-d6f0-4553-95f6-440ddbe4db82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Cleaning up old version at /usr/local/lib/ollama\n",
      ">>> Installing ollama to /usr/local\n",
      ">>> Downloading Linux amd64 bundle\n",
      "############################################################################################# 100.0%\n",
      ">>> Adding ollama user to video group...\n",
      ">>> Adding current user to ollama group...\n",
      ">>> Creating ollama systemd service...\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n",
      "Starting Ollama service...\n",
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling 2049f5674b1e... 100% ▕▏ 9.0 GB                         \u001b[K\n",
      "pulling 66b9ea09bd5b... 100% ▕▏   68 B                         \u001b[K\n",
      "pulling eb4402837c78... 100% ▕▏ 1.5 KB                         \u001b[K\n",
      "pulling 832dd9e00a68... 100% ▕▏  11 KB                         \u001b[K\n",
      "pulling db59b814cab7... 100% ▕▏  488 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "# import time\n",
    "# print(\"Starting Ollama service...\")\n",
    "# time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ollama pull qwen2.5:14b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OEnSj81xljET"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SERPAPI_API_KEY\"] = \"232b686fd513c39f4747dcc3811793b016b07fa7d4da0b62821a3e137bc97657\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DhH7NvVtg67O"
   },
   "source": [
    "## LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDZJnOWrAyBP"
   },
   "source": [
    "Ollama Qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "RKgX_vVacZly"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rahulbhoyar/Projects/DataScience/GenAI-with-Python/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import requests\n",
    "from typing import Literal, TypedDict, List, Dict, Any, Union, Optional, Set, Annotated as AnnotatedType\n",
    "from typing_extensions import Annotated\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# State structure to track which agents have been used\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[List[Dict[str, Any]], \"last_value\"]\n",
    "    next: Optional[str]\n",
    "    query_parts: Dict[str, str]\n",
    "    complete_agents: Set[str]\n",
    "    results: Dict[str, str]\n",
    "    has_math: bool\n",
    "    has_search: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "def get_llm_response(messages, system_prompt=None):\n",
    "    # Initialize the ChatOpenAI model\n",
    "    llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "\n",
    "    # Construct the message list for the model\n",
    "    chat_messages = []\n",
    "    if system_prompt:\n",
    "        chat_messages.append(SystemMessage(content=system_prompt))\n",
    "    chat_messages.extend(HumanMessage(content=msg['content']) for msg in messages)\n",
    "\n",
    "    # Generate the response\n",
    "    response = llm.invoke(chat_messages)\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "CL8M5xXfchC9"
   },
   "outputs": [],
   "source": [
    "# # Initialize the LLM\n",
    "\n",
    "# import ollama\n",
    "\n",
    "\n",
    "# def get_llm_response(messages, system_prompt=None):\n",
    "#     if system_prompt:\n",
    "#         full_messages = [{\"role\": \"system\", \"content\": system_prompt}] + messages\n",
    "#     else:\n",
    "#         full_messages = messages\n",
    "\n",
    "#     response = ollama.chat(\n",
    "#         model=\"qwen2.5:14b\",\n",
    "#         messages=full_messages,\n",
    "#         stream=False\n",
    "#     )\n",
    "#     return response['message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "h8rz2kuhcZeS"
   },
   "outputs": [],
   "source": [
    "def query_analyzer(state: AgentState) -> AgentState:\n",
    "    messages = state[\"messages\"]\n",
    "    user_input = messages[-1][\"content\"]\n",
    "\n",
    "    system_prompt = \"\"\"Analyze the user query and determine:\n",
    "1. If it contains a mathematical calculation\n",
    "2. If it contains a search query or question\n",
    "3. If it contains BOTH a calculation AND a search query\n",
    "\n",
    "For calculations, extract just the mathematical expression.\n",
    "For search queries, extract just the search question.\n",
    "\n",
    "Respond in JSON format:\n",
    "{\n",
    "  \"has_math\": true/false,\n",
    "  \"has_search\": true/false,\n",
    "  \"math_query\": \"extracted math expression or null\",\n",
    "  \"search_query\": \"extracted search question or null\"\n",
    "}\"\"\"\n",
    "\n",
    "    analysis = get_llm_response([{\"role\": \"user\", \"content\": user_input}], system_prompt)\n",
    "\n",
    "    try:\n",
    "        # Parse the JSON response\n",
    "        analysis_json = json.loads(analysis[\"content\"])\n",
    "\n",
    "        new_state = state.copy()\n",
    "        new_state[\"has_math\"] = analysis_json.get(\"has_math\", False)\n",
    "        new_state[\"has_search\"] = analysis_json.get(\"has_search\", False)\n",
    "        new_state[\"query_parts\"] = {}\n",
    "        new_state[\"complete_agents\"] = set()\n",
    "        new_state[\"results\"] = {}\n",
    "\n",
    "        if analysis_json.get(\"has_math\") and analysis_json.get(\"math_query\"):\n",
    "            new_state[\"query_parts\"][\"calculator\"] = analysis_json[\"math_query\"]\n",
    "\n",
    "        if analysis_json.get(\"has_search\") and analysis_json.get(\"search_query\"):\n",
    "            new_state[\"query_parts\"][\"search_engine\"] = analysis_json[\"search_query\"]\n",
    "\n",
    "        if new_state[\"has_math\"] and new_state[\"has_search\"]:\n",
    "            # Start with calculator for composite queries\n",
    "            new_state[\"next\"] = \"calculator\"\n",
    "        elif new_state[\"has_math\"]:\n",
    "            new_state[\"next\"] = \"calculator\"\n",
    "        else:\n",
    "            new_state[\"next\"] = \"search_engine\"\n",
    "\n",
    "        return new_state\n",
    "    except:\n",
    "        # If parsing fails, use the old router logic as fallback\n",
    "        return router_fallback(state)\n",
    "\n",
    "# Fallback to the original router if analysis fails\n",
    "def router_fallback(state: AgentState) -> AgentState:\n",
    "    messages = state[\"messages\"]\n",
    "    user_input = messages[-1][\"content\"]\n",
    "\n",
    "    system_prompt = \"\"\"Determine if the user input is a mathematical calculation or a search query.\n",
    "If it's a mathematical calculation, respond with \"CALCULATOR\".\n",
    "If it's a search query or any other question, respond with \"SEARCH\".\n",
    "Only respond with one word - either \"CALCULATOR\" or \"SEARCH\".\"\"\"\n",
    "\n",
    "    decision = get_llm_response([{\"role\": \"user\", \"content\": user_input}], system_prompt)\n",
    "    decision_text = decision[\"content\"]\n",
    "\n",
    "    new_state = state.copy()\n",
    "    new_state[\"query_parts\"] = {}\n",
    "    new_state[\"complete_agents\"] = set()\n",
    "    new_state[\"results\"] = {}\n",
    "\n",
    "    if \"CALCULATOR\" in decision_text.upper():\n",
    "        new_state[\"has_math\"] = True\n",
    "        new_state[\"has_search\"] = False\n",
    "        new_state[\"query_parts\"][\"calculator\"] = user_input\n",
    "        new_state[\"next\"] = \"calculator\"\n",
    "    else:\n",
    "        new_state[\"has_math\"] = False\n",
    "        new_state[\"has_search\"] = True\n",
    "        new_state[\"query_parts\"][\"search_engine\"] = user_input\n",
    "        new_state[\"next\"] = \"search_engine\"\n",
    "\n",
    "    return new_state\n",
    "\n",
    "def calculator(state: AgentState) -> AgentState:\n",
    "    query = state[\"query_parts\"].get(\"calculator\", state[\"messages\"][-1][\"content\"])\n",
    "\n",
    "    system_prompt = \"\"\"You are a math calculator. Solve the given mathematical expression and show your work.\n",
    "When providing your answer:\n",
    "1. Always include a clear statement of the final result\n",
    "2. Format the final answer as: \"The result is: [answer]\"\n",
    "3. For exponents like 2^10, calculate and show the exact value\n",
    "4. Show each step of your calculation\n",
    "\"\"\"\n",
    "\n",
    "    llm_response = get_llm_response([{\"role\": \"user\", \"content\": query}], system_prompt)\n",
    "\n",
    "    new_state = state.copy()\n",
    "    new_state[\"complete_agents\"].add(\"calculator\")\n",
    "    new_state[\"results\"][\"calculator\"] = f\"Math calculation: {query}\\n\\n{llm_response['content']}\"\n",
    "\n",
    "    if \"search_engine\" in new_state[\"complete_agents\"] or not state[\"has_search\"]:\n",
    "        new_state[\"next\"] = \"response_combiner\"\n",
    "    else:\n",
    "        new_state[\"next\"] = \"search_engine\"\n",
    "\n",
    "    return new_state\n",
    "\n",
    "def search_engine(query: str) -> str:\n",
    "    \"\"\"Perform real web searches using SerpAPI\"\"\"\n",
    "    try:\n",
    "        api_key = os.environ.get(\"SERPAPI_API_KEY\")\n",
    "\n",
    "        if not api_key:\n",
    "            return \"Search error: SERPAPI_API_KEY not found in environment variables\"\n",
    "\n",
    "        # Make API request to SerpAPI\n",
    "        url = \"https://serpapi.com/search\"\n",
    "        params = {\n",
    "            'q': query,\n",
    "            'api_key': api_key,\n",
    "            'engine': 'google',  # Use Google as the search engine\n",
    "            'num': 3  # Number of results\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=params)\n",
    "        results = response.json()\n",
    "\n",
    "        if 'error' in results:\n",
    "            return f\"Search error: {results['error']}\"\n",
    "\n",
    "        if 'organic_results' not in results or not results['organic_results']:\n",
    "            return \"No search results found\"\n",
    "\n",
    "        formatted_results = []\n",
    "        for i, item in enumerate(results['organic_results'][:3], 1):\n",
    "            title = item.get('title', 'No title')\n",
    "            snippet = item.get('snippet', 'No description')\n",
    "            link = item.get('link', 'No URL')\n",
    "            formatted_results.append(\n",
    "                f\"{i}. {title}\\n{snippet}\\nSource: {link}\"\n",
    "            )\n",
    "\n",
    "        return \"\\n\\n\".join(formatted_results)\n",
    "    except Exception as e:\n",
    "        return f\"Search error: {str(e)}\"\n",
    "\n",
    "def search_engine_agent(state: AgentState) -> AgentState:\n",
    "    query = state[\"query_parts\"].get(\"search_engine\", state[\"messages\"][-1][\"content\"])\n",
    "\n",
    "    search_result = search_engine(query)\n",
    "\n",
    "    new_state = state.copy()\n",
    "    new_state[\"complete_agents\"].add(\"search_engine\")\n",
    "    new_state[\"results\"][\"search_engine\"] = f\"Search query: {query}\\n\\n{search_result}\"\n",
    "\n",
    "    # Check if we need to process more components or go to combiner\n",
    "    if \"calculator\" in new_state[\"complete_agents\"] or not state[\"has_math\"]:\n",
    "        new_state[\"next\"] = \"response_combiner\"\n",
    "    else:\n",
    "        new_state[\"next\"] = \"calculator\"\n",
    "\n",
    "    return new_state\n",
    "\n",
    "# Router to decide which agent should handle the query\n",
    "def router(state: AgentState) -> AgentState:\n",
    "    messages = state[\"messages\"]\n",
    "    user_input = messages[-1][\"content\"]\n",
    "\n",
    "    system_prompt = \"\"\"Determine if the user input is a mathematical calculation or a search query.\n",
    "If it's a mathematical calculation, respond with \"CALCULATOR\".\n",
    "If it's a search query or any other question, respond with \"SEARCH\".\n",
    "Only respond with one word - either \"CALCULATOR\" or \"SEARCH\".\"\"\"\n",
    "\n",
    "    decision = get_llm_response([{\"role\": \"user\", \"content\": user_input}], system_prompt)\n",
    "    decision_text = decision[\"content\"]\n",
    "\n",
    "    if \"CALCULATOR\" in decision_text.upper():\n",
    "        return {\"messages\": messages, \"next\": \"calculator\"}\n",
    "    else:\n",
    "        return {\"messages\": messages, \"next\": \"search_engine\"}\n",
    "\n",
    "def response_combiner(state: AgentState) -> AgentState:\n",
    "    system_prompt = \"\"\"You are an assistant that combines results from different tools into a coherent response.\n",
    "You have access to results from a calculator and/or search engine.\n",
    "\n",
    "When both calculation and search results are available:\n",
    "1. Summarize the calculation result clearly\n",
    "2. Extract the most important information from the search results\n",
    "3. Present both pieces of information in a clear, concise format\n",
    "\n",
    "When only calculation results are available:\n",
    "1. Clearly state the result of the calculation\n",
    "2. Include the original expression and the answer\n",
    "\n",
    "When only search results are available:\n",
    "1. Extract the most relevant information from the search results\n",
    "2. Provide a direct answer to the user's question when possible\n",
    "3. If a specific fact is requested (like an exchange rate), state it clearly\n",
    "\n",
    "Keep your response concise and focused on answering the user's query.\"\"\"\n",
    "\n",
    "    combiner_input = \"Here are the results from different tools:\\n\\n\"\n",
    "\n",
    "    if \"calculator\" in state[\"results\"]:\n",
    "        combiner_input += state[\"results\"][\"calculator\"] + \"\\n\\n\"\n",
    "\n",
    "    if \"search_engine\" in state[\"results\"]:\n",
    "        combiner_input += state[\"results\"][\"search_engine\"] + \"\\n\\n\"\n",
    "\n",
    "    combiner_input += f\"Original user query: {state['messages'][-1]['content']}\\n\"\n",
    "    combiner_input += \"Please combine these results into a coherent response that directly answers the user's question.\"\n",
    "\n",
    "    llm_response = get_llm_response([{\"role\": \"user\", \"content\": combiner_input}], system_prompt)\n",
    "\n",
    "    new_messages = state[\"messages\"].copy()\n",
    "    new_messages.append({\"role\": \"assistant\", \"content\": llm_response[\"content\"]})\n",
    "\n",
    "    return {\"messages\": new_messages, \"next\": None,\n",
    "            \"query_parts\": state[\"query_parts\"],\n",
    "            \"complete_agents\": state[\"complete_agents\"],\n",
    "            \"results\": state[\"results\"],\n",
    "            \"has_math\": state[\"has_math\"],\n",
    "            \"has_search\": state[\"has_search\"]}\n",
    "\n",
    "\n",
    "def user_response(state: AgentState) -> AgentState:\n",
    "    return {\"messages\": state[\"messages\"], \"next\": None}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "gNgL9zw7XNtf"
   },
   "outputs": [],
   "source": [
    "# Build the graph\n",
    "def build_graph():\n",
    "    # Define the graph\n",
    "    graph = StateGraph(AgentState)\n",
    "\n",
    "    # Add nodes\n",
    "    graph.add_node(\"query_analyzer\", query_analyzer)\n",
    "    graph.add_node(\"calculator\", calculator)\n",
    "    graph.add_node(\"search_engine\", search_engine_agent)\n",
    "    graph.add_node(\"response_combiner\", response_combiner)\n",
    "\n",
    "    # Connect the graph\n",
    "    graph.add_edge(START, \"query_analyzer\")\n",
    "\n",
    "    # Add conditional edges based on analysis result\n",
    "    graph.add_conditional_edges(\n",
    "        \"query_analyzer\",\n",
    "        lambda state: state[\"next\"],\n",
    "        {\n",
    "            \"calculator\": \"calculator\",\n",
    "            \"search_engine\": \"search_engine\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Add conditional edges from calculator\n",
    "    graph.add_conditional_edges(\n",
    "        \"calculator\",\n",
    "        lambda state: state[\"next\"],\n",
    "        {\n",
    "            \"search_engine\": \"search_engine\",  # Direct link to search engine\n",
    "            \"response_combiner\": \"response_combiner\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Add conditional edges from search engine\n",
    "    graph.add_conditional_edges(\n",
    "        \"search_engine\",\n",
    "        lambda state: state[\"next\"],\n",
    "        {\n",
    "            \"calculator\": \"calculator\",  # Direct link to calculator\n",
    "            \"response_combiner\": \"response_combiner\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    graph.add_edge(\"response_combiner\", END)\n",
    "\n",
    "    # Compile the graph\n",
    "    return graph.compile(name=\"LangGraph Multi-Agent System\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MNQYmwEWcsMx"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    if not os.environ.get(\"SERPAPI_API_KEY\"):\n",
    "        print(\"Please set the SERPAPI_API_KEY environment variable.\")\n",
    "        print(\"You can get it by signing up at https://serpapi.com/\")\n",
    "        return\n",
    "\n",
    "    graph = build_graph()\n",
    "\n",
    "    try:\n",
    "        from google.colab import output\n",
    "        from IPython.display import display, Markdown, HTML\n",
    "        IN_COLAB = True\n",
    "    except ImportError:\n",
    "        IN_COLAB = False\n",
    "\n",
    "    # Process each query\n",
    "    for query in example_queries:\n",
    "        if IN_COLAB:\n",
    "            display(HTML(f\"<h2 style='background:#2c3e50; color:white; padding:10px; border-radius:5px;'>💬 Query: {query}</h2>\"))\n",
    "            display(Markdown(f\"**👤 User:** {query}\\n\"))\n",
    "        else:\n",
    "            print(\"=\" * 60)\n",
    "            print(f\"💬 Query: {query}\")\n",
    "            print(\"=\" * 60)\n",
    "            print(\"\\nResults:\")\n",
    "            print(f\"👤 User: {query}\\n\")\n",
    "\n",
    "        messages = [{\"role\": \"user\", \"content\": query}]\n",
    "\n",
    "        # Run the graph\n",
    "        result = graph.invoke({\n",
    "            \"messages\": messages,\n",
    "            \"next\": None,\n",
    "            \"query_parts\": {},\n",
    "            \"complete_agents\": set(),\n",
    "            \"results\": {},\n",
    "            \"has_math\": False,\n",
    "            \"has_search\": False\n",
    "        })\n",
    "\n",
    "        # Display tool results\n",
    "        if IN_COLAB:\n",
    "            display(HTML(\"<h3 style='background:#34495e; color:white; padding:8px; border-radius:4px;'>🛠️ Tool Results:</h3>\"))\n",
    "        else:\n",
    "            print(\"🛠️ Tool Results:\\n\")\n",
    "\n",
    "        if \"calculator\" in result[\"complete_agents\"]:\n",
    "            calc_content = result[\"results\"][\"calculator\"].split(\"\\n\\n\")[1]\n",
    "\n",
    "            if IN_COLAB:\n",
    "                calc_content = calc_content.replace('\\\\[', '$$')\n",
    "                calc_content = calc_content.replace('\\\\]', '$$')\n",
    "\n",
    "                display(HTML(\"<strong style='color:#16a085; font-size:14px;'>Using calculator:</strong>\"))\n",
    "                display(Markdown(calc_content))\n",
    "            else:\n",
    "                calc_results = calc_content.replace('\\\\[', '').replace('\\\\]', '')\n",
    "                calc_results = calc_results.replace('\\\\boxed{', '').replace('}', '')\n",
    "                print(f\"Using calculator: {calc_results}\\n\")\n",
    "\n",
    "        if \"search_engine\" in result[\"complete_agents\"]:\n",
    "            search_results = result[\"results\"][\"search_engine\"].split(\"\\n\\n\", 1)[1]\n",
    "\n",
    "            if IN_COLAB:\n",
    "                display(HTML(\"<strong style='color:#2980b9; font-size:14px;'>Using search:</strong>\"))\n",
    "                display(Markdown(search_results))\n",
    "            else:\n",
    "                print(f\"Using search:\\n\\n{search_results}\\n\")\n",
    "\n",
    "        if result[\"messages\"][-1][\"role\"] == \"assistant\":\n",
    "            response_content = result[\"messages\"][-1][\"content\"]\n",
    "\n",
    "            if IN_COLAB:\n",
    "                display(HTML(\"<h3 style='background:#27ae60; color:white; padding:8px; border-radius:4px;'>🤖 Assistant's Response:</h3>\"))\n",
    "\n",
    "                display(HTML(f\"\"\"\n",
    "                <div style='background:#f8f9fa; border-left:4px solid #27ae60; padding:15px; border-radius:4px;'>\n",
    "                    <div style='color:#2c3e50; font-size:15px;'>{response_content}</div>\n",
    "                </div>\n",
    "                \"\"\"))\n",
    "            else:\n",
    "                print(f\"🤖 Assistant's Response: {response_content}\\n\")\n",
    "\n",
    "        if IN_COLAB:\n",
    "            display(HTML(\"<hr style='margin:30px 0; border:0; border-top:1px solid #eee;'>\"))\n",
    "        else:\n",
    "            print(\"\\n\" + \"-\" * 60 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "U2qNKw7Ld7xz",
    "outputId": "83ca0b92-86ad-44a6-c91a-6d3cfe84f165"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "💬 Query: What is 2 * 4 + 90?\n",
      "============================================================\n",
      "\n",
      "Results:\n",
      "👤 User: What is 2 * 4 + 90?\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 25\u001b[0m, in \u001b[0;36mquery_analyzer\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# Parse the JSON response\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     analysis_json \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(\u001b[43manalysis\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     27\u001b[0m     new_state \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mcopy()\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m example_queries \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is 2 * 4 + 90?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms the current USD to EUR exchange rate?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalculate 2^10. Also tell me about the langgraph.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m ]\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 9\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 31\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m messages \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: query}]\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Run the graph\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery_parts\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomplete_agents\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresults\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhas_math\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhas_search\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     39\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Display tool results\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m IN_COLAB:\n",
      "File \u001b[0;32m~/Projects/DataScience/GenAI-with-Python/venv/lib/python3.9/site-packages/langgraph/pregel/__init__.py:2683\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2681\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2682\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 2683\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m   2684\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   2685\u001b[0m     config,\n\u001b[1;32m   2686\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[1;32m   2687\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   2688\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   2689\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   2690\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[1;32m   2691\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2692\u001b[0m ):\n\u001b[1;32m   2693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2694\u001b[0m         latest \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[0;32m~/Projects/DataScience/GenAI-with-Python/venv/lib/python3.9/site-packages/langgraph/pregel/__init__.py:2331\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   2325\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   2326\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[1;32m   2327\u001b[0m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   2328\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   2329\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[1;32m   2330\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[0;32m-> 2331\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   2332\u001b[0m             loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m   2333\u001b[0m             timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   2334\u001b[0m             retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[1;32m   2335\u001b[0m             get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   2336\u001b[0m         ):\n\u001b[1;32m   2337\u001b[0m             \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   2338\u001b[0m             \u001b[38;5;28;01myield from\u001b[39;00m output()\n\u001b[1;32m   2339\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/DataScience/GenAI-with-Python/venv/lib/python3.9/site-packages/langgraph/pregel/runner.py:146\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    144\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m                \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m                \u001b[49m\u001b[43mreraise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/Projects/DataScience/GenAI-with-Python/venv/lib/python3.9/site-packages/langgraph/pregel/retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     38\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     42\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[0;32m~/Projects/DataScience/GenAI-with-Python/venv/lib/python3.9/site-packages/langgraph/utils/runnable.py:606\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[1;32m    603\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    604\u001b[0m )\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 606\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/Projects/DataScience/GenAI-with-Python/venv/lib/python3.9/site-packages/langgraph/utils/runnable.py:371\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[0;32m--> 371\u001b[0m         ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[6], line 51\u001b[0m, in \u001b[0;36mquery_analyzer\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_state\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# If parsing fails, use the old router logic as fallback\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrouter_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 64\u001b[0m, in \u001b[0;36mrouter_fallback\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     58\u001b[0m     system_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mDetermine if the user input is a mathematical calculation or a search query.\u001b[39m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124mIf it\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms a mathematical calculation, respond with \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCALCULATOR\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124mIf it\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms a search query or any other question, respond with \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSEARCH\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124mOnly respond with one word - either \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCALCULATOR\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSEARCH\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     63\u001b[0m     decision \u001b[38;5;241m=\u001b[39m get_llm_response([{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: user_input}], system_prompt)\n\u001b[0;32m---> 64\u001b[0m     decision_text \u001b[38;5;241m=\u001b[39m \u001b[43mdecision\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     66\u001b[0m     new_state \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     67\u001b[0m     new_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery_parts\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "example_queries = [\n",
    "    \"What is 2 * 4 + 90?\",\n",
    "    \"What's the current USD to EUR exchange rate?\",\n",
    "    \"Calculate 2^10. Also tell me about the langgraph.\",\n",
    "]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YsMjgf6aXNqr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
